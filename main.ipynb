{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b3f9f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch pandas sklearn yfinance boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b29fd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json, boto3, datetime, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, GRU, Linear, LSTM\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "now = datetime.now() + timedelta(hours=8)\n",
    "dt_string = now.strftime(\"%d_%m_%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "616d8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config_QQQ:\n",
    "\n",
    "    etf = \"QQQ\"\n",
    "\n",
    "    dataset_type = \"qqq\"\n",
    "\n",
    "    feature_columns = list(range(1,7))\n",
    "    label_columns = [2,3]\n",
    "    label_in_feature_index = (lambda x,y: [x.index(i) for i in y])(feature_columns, label_columns)\n",
    "\n",
    "    predict_day = 1\n",
    "\n",
    "    model_type = \"gru\"\n",
    "    train_data_rate = 0.95\n",
    "\n",
    "    do_train = True\n",
    "    do_predict = True\n",
    "    time_step = 20\n",
    "    valid_data_rate = 0.15\n",
    "    random_seed = 42\n",
    "    shuffle_train_data = True\n",
    "\n",
    "    input_size = len(feature_columns)\n",
    "    output_size = len(label_columns)\n",
    "    hidden_size = 128       \n",
    "    layers = 2\n",
    "    dropout_rate = 0.2\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    epoch = 100\n",
    "    do_continue_train = False\n",
    "    do_train_visualized = False\n",
    "    patience = 5\n",
    "    \n",
    "    model_save_path = \"models/\"\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    data_save_path = \"dataset/\"\n",
    "    if not os.path.exists(data_save_path):\n",
    "        os.makedirs(data_save_path)\n",
    "\n",
    "class Data_QQQ:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.data, self.data_column_name = self.read_data()\n",
    "\n",
    "        self.data_num = self.data.shape[0]\n",
    "        self.train_num = int(self.data_num * self.config.train_data_rate)\n",
    "\n",
    "        # Normalization\n",
    "        self.mean = np.mean(self.data, axis=0) \n",
    "        self.std = np.std(self.data, axis=0)\n",
    "        self.norm_data = (self.data - self.mean)/self.std \n",
    "\n",
    "    def read_data(self):\n",
    "        df = yf.download(self.config.etf)\n",
    "        df.to_csv(self.config.data_save_path+self.config.etf+\".csv\")\n",
    "        df = pd.read_csv(self.config.data_save_path+self.config.etf+\".csv\", usecols=self.config.feature_columns)\n",
    "        return df.values, df.columns.tolist()\n",
    "\n",
    "    def get_train_and_valid_data(self):\n",
    "        feature_data = self.norm_data[:self.train_num]\n",
    "        label_data = self.norm_data[self.config.predict_day : self.config.predict_day + self.train_num,\n",
    "                                    self.config.label_in_feature_index]\n",
    "\n",
    "        train_x = [feature_data[start_index + i*self.config.time_step : start_index + (i+1)*self.config.time_step]\n",
    "                        for start_index in range(self.config.time_step)\n",
    "                        for i in range((self.train_num - start_index) // self.config.time_step)]\n",
    "        train_y = [label_data[start_index + i*self.config.time_step : start_index + (i+1)*self.config.time_step]\n",
    "                    for start_index in range(self.config.time_step)\n",
    "                    for i in range((self.train_num - start_index) // self.config.time_step)]\n",
    "\n",
    "        train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, \n",
    "        test_size=self.config.valid_data_rate, random_state=self.config.random_seed, \n",
    "        shuffle=self.config.shuffle_train_data)\n",
    "        \n",
    "        return train_x, valid_x, train_y, valid_y\n",
    "\n",
    "    def get_test_data(self):\n",
    "        feature_data = self.norm_data[self.train_num:]\n",
    "        sample_interval = min(feature_data.shape[0], self.config.time_step)\n",
    "        self.start_num_in_test = feature_data.shape[0] % sample_interval\n",
    "        time_step_size = feature_data.shape[0] // sample_interval\n",
    "\n",
    "        test_x = [feature_data[self.start_num_in_test+i*sample_interval : self.start_num_in_test+(i+1)*sample_interval]\n",
    "                   for i in range(time_step_size)]\n",
    "\n",
    "        return np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4e8af0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config_top10:\n",
    "\n",
    "    etf = \"QQQ\"\n",
    "    tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"TSLA\", \"GOOG\", \"FB\", \"GOOGL\", \"NVDA\", \"PYPL\", \"ADBE\"]\n",
    "\n",
    "    dataset_type = \"top10\"    \n",
    "\n",
    "    feature_columns = list(range(0,22))\n",
    "    label_columns = [20,21]\n",
    "    label_in_feature_index = (lambda x,y: [x.index(i) for i in y])(feature_columns, label_columns)\n",
    "\n",
    "    predict_day = 1\n",
    "\n",
    "    model_type = \"gru\"\n",
    "    train_data_rate = 0.95\n",
    "\n",
    "    do_train = True\n",
    "    do_predict = True\n",
    "    time_step = 20\n",
    "    valid_data_rate = 0.15\n",
    "    random_seed = 42\n",
    "    shuffle_train_data = True\n",
    "\n",
    "    input_size = len(feature_columns)\n",
    "    output_size = len(label_columns)\n",
    "    hidden_size = 128       \n",
    "    layers = 2\n",
    "    dropout_rate = 0.2\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    epoch = 100\n",
    "    do_continue_train = False\n",
    "    do_train_visualized = False\n",
    "    patience = 5\n",
    "\n",
    "    model_save_path = \"models/\"\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    data_save_path = \"dataset/\"\n",
    "    if not os.path.exists(data_save_path):\n",
    "        os.makedirs(data_save_path)\n",
    "\n",
    "class Data_top10:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.data, self.data_column_name = self.read_data()\n",
    "\n",
    "        self.data_num = self.data.shape[0]\n",
    "        self.train_num = int(self.data_num * self.config.train_data_rate)\n",
    "\n",
    "        # Normalization\n",
    "        self.mean = np.mean(self.data, axis=0) \n",
    "        self.std = np.std(self.data, axis=0)\n",
    "        self.norm_data = (self.data - self.mean)/self.std \n",
    "\n",
    "    def read_data(self):\n",
    "        tickers = self.config.tickers\n",
    "        tickers.append(self.config.etf)\n",
    "        ticker_list = yf.Tickers(tickers)\n",
    "        df = {}\n",
    "        for ticker in tickers:\n",
    "            if ticker != \"QQQ\":\n",
    "                df[ticker+\"_High\"] = ticker_list.tickers[ticker].history(start=\"2016-01-01\")['High']\n",
    "                df[ticker+\"_Low\"] = ticker_list.tickers[ticker].history(start=\"2016-01-01\")['Low']\n",
    "            else:\n",
    "                df[\"High\"] = ticker_list.tickers[ticker].history(start=\"2016-01-01\")['High']\n",
    "                df[\"Low\"] = ticker_list.tickers[ticker].history(start=\"2016-01-01\")['Low']\n",
    "        df = pd.DataFrame(df)\n",
    "        df.to_csv(self.config.data_save_path+\"top10\"+\".csv\")\n",
    "        return df.values, df.columns.tolist()\n",
    "\n",
    "    def get_train_and_valid_data(self):\n",
    "        feature_data = self.norm_data[:self.train_num]\n",
    "        label_data = self.norm_data[self.config.predict_day : self.config.predict_day + self.train_num,\n",
    "                                    self.config.label_in_feature_index]\n",
    "\n",
    "        train_x = [feature_data[start_index + i*self.config.time_step : start_index + (i+1)*self.config.time_step]\n",
    "                        for start_index in range(self.config.time_step)\n",
    "                        for i in range((self.train_num - start_index) // self.config.time_step)]\n",
    "        train_y = [label_data[start_index + i*self.config.time_step : start_index + (i+1)*self.config.time_step]\n",
    "                    for start_index in range(self.config.time_step)\n",
    "                    for i in range((self.train_num - start_index) // self.config.time_step)]\n",
    "\n",
    "        train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, \n",
    "        test_size=self.config.valid_data_rate, random_state=self.config.random_seed, \n",
    "        shuffle=self.config.shuffle_train_data)\n",
    "        \n",
    "        return train_x, valid_x, train_y, valid_y\n",
    "\n",
    "    def get_test_data(self):\n",
    "        feature_data = self.norm_data[self.train_num:]\n",
    "        sample_interval = min(feature_data.shape[0], self.config.time_step)\n",
    "        self.start_num_in_test = feature_data.shape[0] % sample_interval\n",
    "        time_step_size = feature_data.shape[0] // sample_interval\n",
    "\n",
    "        test_x = [feature_data[self.start_num_in_test+i*sample_interval : self.start_num_in_test+(i+1)*sample_interval]\n",
    "                   for i in range(time_step_size)]\n",
    "\n",
    "        return np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9361064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(Module):\n",
    "    def __init__(self, input_size, hidden_size, layers, output_size, dropout_rate):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                         num_layers=layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.linear = Linear(in_features=hidden_size, out_features=output_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        linear_out = self.linear(lstm_out)\n",
    "        return linear_out, hidden\n",
    "\n",
    "def lstm_train(config, train_and_valid_data):\n",
    "    \n",
    "    train_X, train_Y, valid_X, valid_Y = train_and_valid_data\n",
    "    \n",
    "    train_X, train_Y = torch.from_numpy(train_X).float(), torch.from_numpy(train_Y).float() # To Tensor\n",
    "    train_loader = DataLoader(TensorDataset(train_X, train_Y), batch_size=config.batch_size, drop_last=True)\n",
    "    \n",
    "    valid_X, valid_Y = torch.from_numpy(valid_X).float(), torch.from_numpy(valid_Y).float() #To Tensor\n",
    "    valid_loader = DataLoader(TensorDataset(valid_X, valid_Y), batch_size=config.batch_size, drop_last=True)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LSTM(config.input_size, config.hidden_size, config.layers, config.output_size, config.dropout_rate).to(device)\n",
    "    \n",
    "    optimizer =  torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    valid_loss_min = float(\"inf\")\n",
    "    bad_epoch = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.epoch):\n",
    "        print(\"Epoch {}/{}\".format(epoch, config.epoch))\n",
    "        model.train()\n",
    "        train_loss_array = []\n",
    "        hidden_train = None\n",
    "        for i, _data in enumerate(train_loader):\n",
    "            _train_X, _train_Y = _data[0].to(device),_data[1].to(device)\n",
    "            optimizer.zero_grad()               \n",
    "            pred_Y, hidden_train = model(_train_X, hidden_train)    \n",
    "            hidden_train = None\n",
    "            #if not config.do_continue_train:\n",
    "            #    hidden_train = None\n",
    "            #else:\n",
    "            #    h_0, c_0 = hidden_train\n",
    "            #    h_0.detach_(), c_0.detach_()    \n",
    "            #    hidden_train = (h_0, c_0)\n",
    "            loss = criterion(pred_Y, _train_Y)  \n",
    "            loss.backward()                     \n",
    "            optimizer.step()                    \n",
    "            train_loss_array.append(loss.item())\n",
    "            global_step += 1\n",
    "            if config.do_train_visualized and global_step % 100 == 0: \n",
    "                vis.line(X=np.array([global_step]), Y=np.array([loss.item()]), win='Train_Loss',\n",
    "                         update='append' if global_step > 0 else None, name='Train', opts=dict(showlegend=True))\n",
    "\n",
    "        model.eval()                    \n",
    "        valid_loss_array = []\n",
    "        hidden_valid = None\n",
    "        for _valid_X, _valid_Y in valid_loader:\n",
    "            _valid_X, _valid_Y = _valid_X.to(device), _valid_Y.to(device)\n",
    "            pred_Y, hidden_valid = model(_valid_X, hidden_valid)\n",
    "            #if not config.do_continue_train: hidden_valid = None\n",
    "            loss = criterion(pred_Y, _valid_Y)  \n",
    "            valid_loss_array.append(loss.item())\n",
    "\n",
    "        train_loss_cur = np.mean(train_loss_array)\n",
    "        valid_loss_cur = np.mean(valid_loss_array)\n",
    "        print(\"The train loss is {:.6f}. \".format(train_loss_cur) +\n",
    "              \"The valid loss is {:.6f}.\".format(valid_loss_cur))\n",
    "        if config.do_train_visualized:      \n",
    "            vis.line(X=np.array([epoch]), Y=np.array([train_loss_cur]), win='Epoch_Loss',\n",
    "                     update='append' if epoch > 0 else None, name='Train', opts=dict(showlegend=True))\n",
    "            vis.line(X=np.array([epoch]), Y=np.array([valid_loss_cur]), win='Epoch_Loss',\n",
    "                     update='append' if epoch > 0 else None, name='Eval', opts=dict(showlegend=True))\n",
    "\n",
    "        if valid_loss_cur < valid_loss_min:\n",
    "            valid_loss_min = valid_loss_cur\n",
    "            bad_epoch = 0\n",
    "            torch.save(model.state_dict(), config.model_save_path + config.model_type + \"_\" + config.dataset_type + \"_model_\"+ dt_string+ \".pth\")\n",
    "        else:\n",
    "            bad_epoch += 1\n",
    "            if bad_epoch >= config.patience:    # Stop training if bad epoch\n",
    "                print(\" The training stops early in epoch {}\".format(epoch))\n",
    "                break\n",
    "\n",
    "def lstm_predict(config, test_X):\n",
    "\n",
    "    test_X = torch.from_numpy(test_X).float()\n",
    "    test_set = TensorDataset(test_X)\n",
    "    test_loader = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LSTM(config.input_size, config.hidden_size, config.layers, config.output_size, config.dropout_rate).to(device)\n",
    "    model.load_state_dict(torch.load(config.model_save_path + config.model_type + \"_\" + config.dataset_type + \"_model_\" + dt_string + \".pth\"))\n",
    "\n",
    "    result = torch.Tensor().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    hidden_predict = None\n",
    "    for _data in test_loader:\n",
    "        data_X = _data[0].to(device)\n",
    "        pred_X, hidden_predict = model(data_X, hidden_predict)\n",
    "        # if not config.do_continue_train: hidden_predict = None \n",
    "        cur_pred = torch.squeeze(pred_X, dim=0)\n",
    "        result = torch.cat((result, cur_pred), dim=0)\n",
    "\n",
    "    return result.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1232de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(Module):\n",
    "    def __init__(self, input_size, hidden_size, layers, output_size, dropout_rate):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        #h0 = torch.zeros(self.layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        gru_out, hidden = self.gru(x, hidden)\n",
    "        out = self.fc(gru_out) \n",
    "        return out, hidden\n",
    "\n",
    "def gru_train(config, train_and_valid_data):\n",
    "    train_X, train_Y, valid_X, valid_Y = train_and_valid_data\n",
    "    \n",
    "    train_X, train_Y = torch.from_numpy(train_X).float(), torch.from_numpy(train_Y).float() # To Tensor\n",
    "    train_loader = DataLoader(TensorDataset(train_X, train_Y), batch_size=config.batch_size, drop_last=True)\n",
    "    \n",
    "    valid_X, valid_Y = torch.from_numpy(valid_X).float(), torch.from_numpy(valid_Y).float() #To Tensor\n",
    "    valid_loader = DataLoader(TensorDataset(valid_X, valid_Y), batch_size=config.batch_size, drop_last=True)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GRU(config.input_size, config.hidden_size, config.layers, config.output_size, config.dropout_rate).to(device)\n",
    "    \n",
    "    optimizer =  torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    valid_loss_min = float(\"inf\")\n",
    "    bad_epoch = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.epoch):\n",
    "        print(\"Epoch {}/{}\".format(epoch, config.epoch))\n",
    "        model.train()\n",
    "        train_loss_array = []\n",
    "        hidden_train = None\n",
    "        for i, _data in enumerate(train_loader):\n",
    "            _train_X, _train_Y = _data[0].to(device),_data[1].to(device)\n",
    "            optimizer.zero_grad()               \n",
    "            pred_Y, hidden_train = model(_train_X, hidden_train)    \n",
    "            hidden_train = None\n",
    "            #if not config.do_continue_train:\n",
    "            #    hidden_train = None\n",
    "            #else:\n",
    "            #    h_0, c_0 = hidden_train\n",
    "            #    h_0.detach_(), c_0.detach_()    \n",
    "            #    hidden_train = (h_0, c_0)\n",
    "            loss = criterion(pred_Y, _train_Y)  \n",
    "            loss.backward()                     \n",
    "            optimizer.step()                    \n",
    "            train_loss_array.append(loss.item())\n",
    "            global_step += 1\n",
    "            if config.do_train_visualized and global_step % 100 == 0: \n",
    "                vis.line(X=np.array([global_step]), Y=np.array([loss.item()]), win='Train_Loss',\n",
    "                         update='append' if global_step > 0 else None, name='Train', opts=dict(showlegend=True))\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss_array = []\n",
    "        hidden_valid = None\n",
    "        for _valid_X, _valid_Y in valid_loader:\n",
    "            _valid_X, _valid_Y = _valid_X.to(device), _valid_Y.to(device)\n",
    "            pred_Y, hidden_valid = model(_valid_X, hidden_valid)\n",
    "            #if not config.do_continue_train: hidden_valid = None\n",
    "            loss = criterion(pred_Y, _valid_Y)  \n",
    "            valid_loss_array.append(loss.item())\n",
    "\n",
    "        train_loss_cur = np.mean(train_loss_array)\n",
    "        valid_loss_cur = np.mean(valid_loss_array)\n",
    "        print(\"The train loss is {:.6f}. \".format(train_loss_cur) +\n",
    "              \"The valid loss is {:.6f}.\".format(valid_loss_cur))\n",
    "        if config.do_train_visualized:      \n",
    "            vis.line(X=np.array([epoch]), Y=np.array([train_loss_cur]), win='Epoch_Loss',\n",
    "                     update='append' if epoch > 0 else None, name='Train', opts=dict(showlegend=True))\n",
    "            vis.line(X=np.array([epoch]), Y=np.array([valid_loss_cur]), win='Epoch_Loss',\n",
    "                     update='append' if epoch > 0 else None, name='Eval', opts=dict(showlegend=True))\n",
    "\n",
    "        if valid_loss_cur < valid_loss_min:\n",
    "            valid_loss_min = valid_loss_cur\n",
    "            bad_epoch = 0\n",
    "            torch.save(model.state_dict(), config.model_save_path + config.model_type + \"_\" + config.dataset_type + \"_model_\" + dt_string + \".pth\")\n",
    "        else:\n",
    "            bad_epoch += 1\n",
    "            if bad_epoch >= config.patience:    # Stop training if bad epoch\n",
    "                print(\" The training stops early in epoch {}\".format(epoch))\n",
    "                break\n",
    "\n",
    "def gru_predict(config, test_X):\n",
    "\n",
    "    test_X = torch.from_numpy(test_X).float()\n",
    "    test_set = TensorDataset(test_X)\n",
    "    test_loader = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GRU(config.input_size, config.hidden_size, config.layers, config.output_size, config.dropout_rate).to(device)\n",
    "    model.load_state_dict(torch.load(config.model_save_path + config.model_type + \"_\" + config.dataset_type + \"_model_\" + dt_string + \".pth\"))\n",
    "\n",
    "    result = torch.Tensor().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    hidden_predict = None\n",
    "    for _data in test_loader:\n",
    "        data_X = _data[0].to(device)\n",
    "        pred_X, hidden_predict = model(data_X, hidden_predict)\n",
    "        # if not config.do_continue_train: hidden_predict = None \n",
    "        cur_pred = torch.squeeze(pred_X, dim=0)\n",
    "        result = torch.cat((result, cur_pred), dim=0)\n",
    "\n",
    "    return result.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be7767ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "\n",
    "    config_dict = {}\n",
    "    for key in dir(config):\n",
    "        if not key.startswith(\"_\"):\n",
    "            config_dict[key] = getattr(config, key)\n",
    "            \n",
    "    if config.dataset_type == \"qqq\":\n",
    "        dataset = Data_QQQ(config)\n",
    "    elif config.dataset_type == \"top10\":\n",
    "        dataset = Data_top10(config)\n",
    "        \n",
    "    if config.do_train:\n",
    "        train_X, valid_X, train_Y, valid_Y = dataset.get_train_and_valid_data()\n",
    "        if config.model_type == \"gru\":\n",
    "            gru_train(config, [train_X, train_Y, valid_X, valid_Y])\n",
    "        elif config.model_type == \"lstm\":\n",
    "            lstm_train(config, [train_X, train_Y, valid_X, valid_Y])\n",
    "\n",
    "    if config.do_predict:\n",
    "        test_X = dataset.get_test_data()\n",
    "        if config.model_type == \"gru\":\n",
    "            pred_result = gru_predict(config, test_X)\n",
    "        elif config.model_type == \"lstm\":\n",
    "            pred_result = lstm_predict(config, test_X)\n",
    "    \n",
    "    label_data = dataset.data[dataset.train_num + dataset.start_num_in_test : ,\n",
    "                                            config.label_in_feature_index]\n",
    "    predict_data = pred_result * dataset.std[config.label_in_feature_index] + \\\n",
    "                dataset.mean[config.label_in_feature_index]\n",
    "\n",
    "    label_name = [dataset.data_column_name[i] for i in config.label_in_feature_index]\n",
    "    label_column_num = len(config.label_columns)\n",
    "\n",
    "    loss = np.mean((label_data[config.predict_day:] - predict_data[:-config.predict_day] ) ** 2, axis=0)\n",
    "    loss_norm = loss/(dataset.std[config.label_in_feature_index] ** 2)\n",
    "    print(\"The mean squared error of stock {} is \".format(label_name) + str(loss_norm))\n",
    "\n",
    "    label_X = range(dataset.data_num - dataset.train_num - dataset.start_num_in_test)\n",
    "    predict_X = [ x + config.predict_day for x in label_X]\n",
    "\n",
    "    for i in range(label_column_num):\n",
    "        try:\n",
    "            config_dict[label_name[i]] = int(np.squeeze(predict_data[-config.predict_day:, i]))\n",
    "        except:\n",
    "            a = list(np.squeeze(predict_data[-config.predict_day:, i]))\n",
    "            a.reverse()\n",
    "            config_dict[label_name[i]] = a\n",
    "        print(\"The predicted stock {} for the next {} day(s) is: \".format(label_name[i],\n",
    "        config.predict_day) + str(np.squeeze(predict_data[-config.predict_day:, i])))\n",
    "        \n",
    "    path_log = \"log/\"+config.dataset_type+\"_\"+config.model_type+\"_\"+dt_string+\".json\"\n",
    "    path_model = \"models/\" + config.model_type+ \"_\" + config.dataset_type + \"_model_\" + dt_string + \".pth\"\n",
    "    path_dataset = \"dataset/\"\n",
    "    with open(path_log, \"w\") as outfile:\n",
    "        json.dump(config_dict, outfile, indent = 1)\n",
    "    \n",
    "    bucket = \"stockpricestorage\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.meta.client.upload_file(path_log, bucket, path_log)\n",
    "    s3.meta.client.upload_file(path_model, bucket, path_model)\n",
    "    for root,dirs,files in os.walk(path_dataset):\n",
    "        for file in files:\n",
    "            s3.meta.client.upload_file(os.path.join(root,file),bucket, path_dataset+file)\n",
    "    print(\"successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "80ac543c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "The train loss is 0.306936. The valid loss is 0.132651.\n",
      "Epoch 1/100\n",
      "The train loss is 0.080980. The valid loss is 0.074543.\n",
      "Epoch 2/100\n",
      "The train loss is 0.037447. The valid loss is 0.040133.\n",
      "Epoch 3/100\n",
      "The train loss is 0.024397. The valid loss is 0.030769.\n",
      "Epoch 4/100\n",
      "The train loss is 0.017951. The valid loss is 0.024713.\n",
      "Epoch 5/100\n",
      "The train loss is 0.014027. The valid loss is 0.020242.\n",
      "Epoch 6/100\n",
      "The train loss is 0.011112. The valid loss is 0.016767.\n",
      "Epoch 7/100\n",
      "The train loss is 0.008838. The valid loss is 0.013912.\n",
      "Epoch 8/100\n",
      "The train loss is 0.006915. The valid loss is 0.011589.\n",
      "Epoch 9/100\n",
      "The train loss is 0.005388. The valid loss is 0.009912.\n",
      "Epoch 10/100\n",
      "The train loss is 0.004352. The valid loss is 0.008562.\n",
      "Epoch 11/100\n",
      "The train loss is 0.003698. The valid loss is 0.007726.\n",
      "Epoch 12/100\n",
      "The train loss is 0.003151. The valid loss is 0.007172.\n",
      "Epoch 13/100\n",
      "The train loss is 0.002860. The valid loss is 0.006786.\n",
      "Epoch 14/100\n",
      "The train loss is 0.002713. The valid loss is 0.006500.\n",
      "Epoch 15/100\n",
      "The train loss is 0.002508. The valid loss is 0.006610.\n",
      "Epoch 16/100\n",
      "The train loss is 0.002460. The valid loss is 0.006606.\n",
      "Epoch 17/100\n",
      "The train loss is 0.002314. The valid loss is 0.006677.\n",
      "Epoch 18/100\n",
      "The train loss is 0.002314. The valid loss is 0.006906.\n",
      "Epoch 19/100\n",
      "The train loss is 0.002216. The valid loss is 0.006955.\n",
      " The training stops early in epoch 19\n",
      "The mean squared error of stock ['High', 'Low'] is [0.02491607 0.02735629]\n",
      "The predicted stock High for the next 1 day(s) is: 360.2047219803014\n",
      "The predicted stock Low for the next 1 day(s) is: 350.35640629075584\n",
      "successful\n",
      "The mean squared error of stock ['High', 'Low'] is [0.03943987 0.04694244]\n",
      "The predicted stock High for the next 5 day(s) is: [379.9933967  370.16888684 370.66300926 368.01699447 360.20472151]\n",
      "The predicted stock Low for the next 5 day(s) is: [370.90663442 360.75574524 361.18420805 358.43354246 350.35640663]\n",
      "successful\n",
      "Epoch 0/100\n",
      "The train loss is 0.170785. The valid loss is 0.053693.\n",
      "Epoch 1/100\n",
      "The train loss is 0.035536. The valid loss is 0.029086.\n",
      "Epoch 2/100\n",
      "The train loss is 0.019090. The valid loss is 0.019947.\n",
      "Epoch 3/100\n",
      "The train loss is 0.012467. The valid loss is 0.016550.\n",
      "Epoch 4/100\n",
      "The train loss is 0.009151. The valid loss is 0.014713.\n",
      "Epoch 5/100\n",
      "The train loss is 0.006954. The valid loss is 0.012921.\n",
      "Epoch 6/100\n",
      "The train loss is 0.005525. The valid loss is 0.011094.\n",
      "Epoch 7/100\n",
      "The train loss is 0.004495. The valid loss is 0.008908.\n",
      "Epoch 8/100\n",
      "The train loss is 0.003911. The valid loss is 0.007846.\n",
      "Epoch 9/100\n",
      "The train loss is 0.003455. The valid loss is 0.006608.\n",
      "Epoch 10/100\n",
      "The train loss is 0.003156. The valid loss is 0.005779.\n",
      "Epoch 11/100\n",
      "The train loss is 0.002898. The valid loss is 0.005144.\n",
      "Epoch 12/100\n",
      "The train loss is 0.002742. The valid loss is 0.004669.\n",
      "Epoch 13/100\n",
      "The train loss is 0.002605. The valid loss is 0.004606.\n",
      "Epoch 14/100\n",
      "The train loss is 0.002553. The valid loss is 0.004369.\n",
      "Epoch 15/100\n",
      "The train loss is 0.002425. The valid loss is 0.004508.\n",
      "Epoch 16/100\n",
      "The train loss is 0.002376. The valid loss is 0.004223.\n",
      "Epoch 17/100\n",
      "The train loss is 0.002263. The valid loss is 0.004303.\n",
      "Epoch 18/100\n",
      "The train loss is 0.002201. The valid loss is 0.004322.\n",
      "Epoch 19/100\n",
      "The train loss is 0.002131. The valid loss is 0.004495.\n",
      "Epoch 20/100\n",
      "The train loss is 0.002097. The valid loss is 0.004592.\n",
      "Epoch 21/100\n",
      "The train loss is 0.002065. The valid loss is 0.004809.\n",
      " The training stops early in epoch 21\n",
      "The mean squared error of stock ['High', 'Low'] is [0.02022443 0.03865634]\n",
      "The predicted stock High for the next 1 day(s) is: 365.0559012348417\n",
      "The predicted stock Low for the next 1 day(s) is: 360.1798541138839\n",
      "successful\n",
      "The mean squared error of stock ['High', 'Low'] is [0.03338382 0.05552615]\n",
      "The predicted stock High for the next 5 day(s) is: [379.07378996 373.96311036 372.20590848 370.16226312 365.05591171]\n",
      "The predicted stock Low for the next 5 day(s) is: [375.22320596 369.57098931 367.69662294 365.62366946 360.17987734]\n",
      "successful\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0/100\n",
      "The train loss is 0.101561. The valid loss is 0.033532.\n",
      "Epoch 1/100\n",
      "The train loss is 0.013527. The valid loss is 0.013379.\n",
      "Epoch 2/100\n",
      "The train loss is 0.005657. The valid loss is 0.005552.\n",
      "Epoch 3/100\n",
      "The train loss is 0.002777. The valid loss is 0.003073.\n",
      "Epoch 4/100\n",
      "The train loss is 0.001818. The valid loss is 0.002531.\n",
      "Epoch 5/100\n",
      "The train loss is 0.001455. The valid loss is 0.002662.\n",
      "Epoch 6/100\n",
      "The train loss is 0.001276. The valid loss is 0.003119.\n",
      "Epoch 7/100\n",
      "The train loss is 0.001169. The valid loss is 0.003324.\n",
      "Epoch 8/100\n",
      "The train loss is 0.001083. The valid loss is 0.003608.\n",
      "Epoch 9/100\n",
      "The train loss is 0.000970. The valid loss is 0.003996.\n",
      " The training stops early in epoch 9\n",
      "The mean squared error of stock ['High', 'Low'] is [0.01022575 0.01085586]\n",
      "The predicted stock High for the next 1 day(s) is: 347.48947017533055\n",
      "The predicted stock Low for the next 1 day(s) is: 341.46326742874766\n",
      "successful\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The mean squared error of stock ['High', 'Low'] is [0.01895262 0.0226741 ]\n",
      "The predicted stock High for the next 5 day(s) is: [360.50398344 354.84480943 355.10374308 352.60187152 347.48947018]\n",
      "The predicted stock Low for the next 5 day(s) is: [354.88443451 348.87460277 349.24632932 346.68279633 341.46326743]\n",
      "successful\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0/100\n",
      "The train loss is 0.057252. The valid loss is 0.013881.\n",
      "Epoch 1/100\n",
      "The train loss is 0.007191. The valid loss is 0.004559.\n",
      "Epoch 2/100\n",
      "The train loss is 0.002977. The valid loss is 0.003452.\n",
      "Epoch 3/100\n",
      "The train loss is 0.001825. The valid loss is 0.004688.\n",
      "Epoch 4/100\n",
      "The train loss is 0.001414. The valid loss is 0.007732.\n",
      "Epoch 5/100\n",
      "The train loss is 0.001151. The valid loss is 0.014113.\n",
      "Epoch 6/100\n",
      "The train loss is 0.001009. The valid loss is 0.021969.\n",
      "Epoch 7/100\n",
      "The train loss is 0.000973. The valid loss is 0.027297.\n",
      " The training stops early in epoch 7\n",
      "The mean squared error of stock ['High', 'Low'] is [0.02959278 0.03805237]\n",
      "The predicted stock High for the next 1 day(s) is: 343.0050112878517\n",
      "The predicted stock Low for the next 1 day(s) is: 333.93828803311675\n",
      "successful\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The mean squared error of stock ['High', 'Low'] is [0.03806361 0.04947341]\n",
      "The predicted stock High for the next 5 day(s) is: [353.2649943  347.573656   350.13408722 347.40170154 343.00501129]\n",
      "The predicted stock Low for the next 5 day(s) is: [344.04702419 338.31238736 340.81255651 338.2183488  333.93828803]\n",
      "successful\n"
     ]
    }
   ],
   "source": [
    "config_QQQ = Config_QQQ()\n",
    "config_top10 = Config_top10()\n",
    "\n",
    "config_top10.predict_day = 1\n",
    "config_top10.do_train = True\n",
    "config_top10.model_type = \"lstm\"\n",
    "main(config_top10)\n",
    "\n",
    "config_top10.do_train = False\n",
    "config_top10.predict_day = 5\n",
    "main(config_top10)\n",
    "\n",
    "config_top10.predict_day = 1\n",
    "config_top10.do_train = True\n",
    "config_top10.model_type = \"gru\"\n",
    "main(config_top10)\n",
    "\n",
    "config_top10.do_train = False\n",
    "config_top10.predict_day = 5\n",
    "main(config_top10)\n",
    "\n",
    "config_QQQ.predict_day = 1\n",
    "config_QQQ.do_train = True\n",
    "config_QQQ.model_type = \"lstm\"\n",
    "main(config_QQQ)\n",
    "\n",
    "config_QQQ.do_train = False\n",
    "config_QQQ.predict_day = 5\n",
    "main(config_QQQ)\n",
    "\n",
    "config_QQQ.predict_day = 1\n",
    "config_QQQ.do_train = True\n",
    "config_QQQ.model_type = \"gru\"\n",
    "main(config_QQQ)\n",
    "\n",
    "config_QQQ.do_train = False\n",
    "config_QQQ.predict_day = 5\n",
    "main(config_QQQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fa8bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362.17422497367517, 363.50279216060835, 365.2884136228962, 373.5453566695867, 374.0302403779073]\n",
      "[355.27021572893545, 356.5110978851526, 358.26301885823455, 367.1580410150733, 367.73987227887596]\n",
      "[372.20544977015396, 373.88691447076667, 375.78376903901506, 379.9837392390169, 379.625225062636]\n",
      "[362.8578829071415, 364.6666205821832, 366.64578759492565, 371.1633071925974, 370.8861596428284]\n",
      "[352.7714810938003, 354.80066519363845, 353.81135905293064, 360.3394525770869, 364.29635348028035]\n",
      "[345.5359058173793, 347.41968261296836, 346.00437849601826, 353.6912449948805, 358.1363140426743]\n",
      "[351.9413750374864, 353.5431665854726, 353.1295700256862, 356.024098624915, 356.81275601884977]\n",
      "[346.3077974709553, 347.9521955705584, 347.59761213995137, 351.15979037031025, 352.1029543914639]\n"
     ]
    }
   ],
   "source": [
    "with open('./log/top10_lstm_10_04_22.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data['High'])\n",
    "    print(data['Low'])\n",
    "with open('./log/top10_gru_10_04_22.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data['High'])\n",
    "    print(data['Low'])\n",
    "with open('./log/qqq_lstm_10_04_22.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data['High'])\n",
    "    print(data['Low'])\n",
    "with open('./log/qqq_gru_10_04_22.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data['High'])\n",
    "    print(data['Low'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77f363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e0403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63432efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260bcead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fac5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088467ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3cae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf554e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408d3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105bbdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
